@article{aceitunoTheoreticalPrinciplesExplain2024,
  title = {Theoretical Principles Explain the Structure of the Insect Head Direction Circuit},
  author = {Aceituno, Pau and Dall'Osto, Dominic and Pisokas, Ioannis},
  editor = {Colgin, Laura L and Vafidis, Pantelis},
  year = {2024},
  month = may,
  journal = {eLife},
  volume = {13},
  pages = {e91533},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.91533},
  urldate = {2024-05-30},
  abstract = {To navigate their environment, insects need to keep track of their orientation. Previous work has shown that insects encode their head direction as a sinusoidal activity pattern around a ring of neurons arranged in an eight-column structure. However, it is unclear whether this sinusoidal encoding of head direction is just an evolutionary coincidence or if it offers a particular functional advantage. To address this question, we establish the basic mathematical requirements for direction encoding and show that it can be performed by many circuits, all with different activity patterns. Among these activity patterns, we prove that the sinusoidal one is the most noise-resilient, but only when coupled with a sinusoidal connectivity pattern between the encoding neurons. We compare this predicted optimal connectivity pattern with anatomical data from the head direction circuits of the locust and the fruit fly, finding that our theory agrees with experimental evidence. Furthermore, we demonstrate that our predicted circuit can emerge using Hebbian plasticity, implying that the neural connectivity does not need to be explicitly encoded in the genetic program of the insect but rather can emerge during development. Finally, we illustrate that in our theory, the consistent presence of the eight-column organisation of head direction circuits across multiple insect species is not a chance artefact but instead can be explained by basic evolutionary principles.},
  pmid = {https://pubmed.ncbi.nlm.nih.gov/38814703},
  keywords = {compass system,head direction cells,Hebbian plasticity,navigation},
  annotation = {OpenAlex: W4399174184},
}

@inproceedings{dallostoAutomaticCalibrationBiologically2018,
  title = {Automatic {{Calibration}} of a {{Biologically Inspired Neural Network}} for {{Robot SLAM}}},
  booktitle = {Australasian {{Conference}} on {{Robotics}} and {{Automation}} 2018},
  author = {Dall'Osto, Dominic and Hausler, Stephen and Jacobson, Adam and Milford, Michael},
  year = {2018},
  address = {Christchurch},
  abstract = {Neural networks have long been a promising model for creating high performance robotic systems, from robot navigation and SLAM to modern deep learning techniques for tasks like manipulation. Traditional neural network systems typically relied heavily on a large number of hand-tuned parameters, while many modern implementations perform end-to-end learning, often with extreme data and computational requirements. Past work has focused on achieving high performance in real world environments, but with extensive hand tuning. In this paper, we instead present a new framework for automatically calibrating and optimising the performance of a biologically inspired neural network SLAM system. This framework combines a preset network structure with learning procedures. We use simulations with realistic noise to demonstrate the system's ability to learn the basic components of SLAM: odometry integration, landmark learning and landmarkdriven relocalisation. We also show the framework is able to calibrate a large range of network sizes, allowing rapid development and deployment of a bio-inspired SLAM system. Our work serves as a bridging contribution between traditional hand-crafted neural networks and modern end-to-end learning approaches.},
  copyright = {All rights reserved},
  isbn = {978-1-5108-7958-4},
  langid = {english},
  annotation = {QID: Q107412014\\
MAG: 3019940072\\
CorpusID: 226916647\\
OpenAlex: W3019940072},
}

@inproceedings{dallostoFastRobustBioinspired2021,
  title = {Fast and {{Robust Bio-inspired Teach}} and {{Repeat Navigation}}},
  booktitle = {2021 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Dall'Osto, Dominic and Fischer, Tobias and Milford, Michael},
  year = {2021},
  month = sep,
  eprint = {2010.11326},
  pages = {500--507},
  publisher = {IEEE},
  address = {Prague, Czech Republic},
  doi = {10.1109/IROS51168.2021.9636334},
  urldate = {2022-02-03},
  abstract = {Fully autonomous mobile robots have a multitude of potential applications, but guaranteeing robust navigation performance remains an open research problem. For many tasks such as repeated infrastructure inspection, item delivery, or inventory transport, a route repeating capability can be sufficient and offers potential practical advantages over a full navigation stack. Previous teach and repeat research has achieved high performance in difficult conditions predominantly by using sophisticated, expensive sensors, and has often had high computational requirements. Biological systems, such as small animals and insects like seeing ants, offer a proof of concept that robust and generalisable navigation can be achieved with extremely limited visual systems and computing power. In this work we create a novel asynchronous formulation for teach and repeat navigation that fully utilises odometry information, paired with a correction signal driven by much more computationally lightweight visual processing than is typically required. This correction signal is also decoupled from the robot's motor control, allowing its rate to be modulated by the available computing capacity. We evaluate this approach with extensive experimentation on two different robotic platforms, the Consequential Robotics Miro and the Clearpath Jackal robots, across navigation trials totalling more than 6000 metres in a range of challenging indoor and outdoor environments. Our approach continues to succeed when multiple state-of-the-art systems fail due to low resolution images, unreliable odometry, or lighting change, while requiring significantly less compute. We also - for the first time - demonstrate versatile cross-platform teach and repeat without changing parameters, in which we learn to navigate a route with one robot and repeat that route using a completely different robot.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  isbn = {978-1-6654-1714-3},
  annotation = {QID: Q110801790\\
MAG: 3093966866\\
CorpusID: 225040111\\
OpenAlex: W4205172893},
}
